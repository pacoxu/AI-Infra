# PD Disaggregation in LLM Inference

This document introduces the concept of Prefill–Decode (PD) disaggregation in large language model (LLM) inference, its benefits, current implementation status in common AI Infra projects, and the future roadmap for its adoption.

## Table of Contents

- [What is PD Disaggregation](#what-is-pd-disaggregation)
- [Project Support Status](#project-support-status)
  - [NVIDIA Dynamo](#nvidia-dynamo)
  - [vLLM production stack](#vllm-production-stack)
  - [AIBrix](#aibrix)
  - [InftyAI/llmaz](#inftyaillmaz)
- [References](#references)

---

## What is PD Disaggregation

In LLM inference, the process can be divided into two distinct phases:

- **Prefill**: processes the entire input prompt in parallel, builds KV cache.
- **Decode**: generates output tokens one by one using the KV cache.

In a monolithic setup, both prefill and decode run on the same GPU, which causes resource contention (e.g., prefill latency impacting decode throughput). **PD Disaggregation** addresses this by disaggregating prefill and decode phases into separate GPU workers or nodes.

**Benefits:**

- Improved latency (TTFT and TPOT)
- Higher throughput per GPU
- Independent scaling and tuning of each phase
- Flexibility for scheduling and load balancing

---

## Project Support Status

### NVIDIA Dynamo

- [FEATURE]: Unifying Disagg Implementation in Dynamo [#1728](https://github.com/ai-dynamo/dynamo/issues/1728)

### vLLM production stack

- vLLM community is exploring deeper native integration in https://github.com/vllm-project/production-stack/pull/340.

### AIBrix

- WIP issue: [Add Prefill/Decode Disaggregation Support in Inference Gateway](https://github.com/vllm-project/aibrix/issues/1223) and [#958](https://github.com/vllm-project/aibrix/issues/958).

### InftyAI/llmaz

- Not supported yet.
- Milestone [v0.3.0](https://github.com/InftyAI/llmaz/issues/433) includes PD disaggregation.

## References

- https://github.com/ai-dynamo/dynamo
- https://github.com/vllm-project/vllm
- https://github.com/vllm-project/production-stack
- https://github.com/vllm-project/aibrix
- https://github.com/InftyAI/llmaz
- https://github.com/LMCache/lmcache
- DistServe (OSDI’24): https://www.usenix.org/system/files/osdi24-zhong-yinmin.pdf

**Some were generated by ChatGPT. So please be careful before you use them. This is a personal learning notes.**