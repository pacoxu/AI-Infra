---
status: Active
maintainer: pacoxu
date: 2025-12-05
tags: kubernetes, ai-infrastructure, gpu, large-scale, multi-cluster, survey
canonical_path: docs/blog/2025-12-05/ai-infrastructure-kubernetes-2025_zh.md
source: https://kube.today/ai-infrastructure-2025
---

# Kubernetes 上的 AI 基础设施：2025 年调研洞察与最佳实践

随着 AI 工作负载越来越多地迁移到 Kubernetes，了解真实世界的挑战和部署模式变得至关重要。
本文探讨了最近关于在 Kubernetes 上大规模运行 AI 基础设施的调研和行业观察的关键洞察。

## 关键调研发现

最近对运行 AI 工作负载的 Kubernetes 实践者的调研揭示了关于 AI 基础设施现状的惊人洞察：

### 大规模集群的现实检验

**只有 12% 的受访者在生产环境中实际接触过 10,000+ 节点的集群。** 这一统计数据突显了
行业炒作与实际现实之间的重要差距：

- 大多数组织运行的规模要小得多（100-1000 个节点）
- "超大规模"演讲中讨论的挑战可能不适用于大多数人
- 应该关注适合典型集群规模的实用优化
- 大规模部署的经验教训仍然提供有价值的见解

### GPU 利用率挑战

GPU 利用率仍然是 AI 基础设施中最关键的挑战之一：

**常见利用率模式：**

- **开发/测试**: 10-30% GPU 利用率
- **训练工作负载**: 40-70% GPU 利用率
- **推理工作负载**: 20-50% GPU 利用率
- **业界最佳**: 70-85% GPU 利用率

**识别的关键挑战：**

1. **资源碎片化**
   - Pod 请求完整 GPU 但只使用一小部分
   - 缺乏 GPU 共享和时间切片采用
   - 由于拓扑约束导致装箱效率低

2. **任务之间的空闲时间**
   - 任务队列延迟和调度延迟
   - 镜像拉取和 Pod 启动时间慢
   - 需要人工干预

3. **开发效率低下**
   - 交互式 Notebook 占用空闲 GPU
   - 分配 GPU 的长调试周期
   - 缺乏对低优先级工作的抢占

**改进策略：**

- 实施 GPU 共享（MIG、时间切片、vGPU）
- 使用 Spot/可抢占实例进行训练
- 采用动态资源分配（DRA）
- 实施 GPU 池化和分数分配
- 在空闲期间启用自动缩容

### 训练和推理工作负载分离

**为什么分离很重要：**

调研突出显示，**训练-推理工作负载分离**作为最佳实践的采用率正在增长：

**训练特性：**

- 具有高 GPU 利用率突发的批处理
- 长时间运行的任务（数小时到数周）
- 具有检查点的容错能力
- 需要 Gang 调度和多节点协调
- 网络密集型（all-reduce、集体操作）

**推理特性：**

- 具有严格延迟 SLO 的实时服务
- 持续运行（24/7）
- 需要高可用性和自动扩展
- 专注于吞吐量和每次查询成本
- 可以利用模型缓存和批处理

**分离的好处：**

1. **资源优化**
   - 训练集群：优化吞吐量和成本
   - 推理集群：优化延迟和可用性
   - 每种工作负载使用不同的 GPU 类型（训练用 A100，推理用 T4）

2. **运营效率**
   - 独立的扩展策略
   - 单独的升级窗口
   - 隔离的故障域
   - 不同的监控和告警策略

3. **成本管理**
   - 训练使用 Spot 实例（节省 70% 成本）
   - 推理使用预留实例（可预测成本）
   - 按工作负载类型进行合理配置

**实施模式：**

```yaml
# 训练集群配置
- 节点池: 带 NVLink/InfiniBand 的 GPU 优化
- 调度器: Kueue、Volcano 与 Gang 调度
- 存储: 高带宽共享文件系统
- 网络: 支持 RDMA 的分布式训练
- 工作负载: Kubeflow、PyTorch Elastic、DeepSpeed

# 推理集群配置
- 节点池: 带自动扩展的混合 GPU/CPU
- 服务: KServe、Seldon、vLLM、TensorRT
- 存储: 带快速本地缓存的模型注册表
- 网络: 带流量整形的服务网格
- 工作负载: REST API、gRPC 端点
```

## AI 基础设施架构模式

### 单集群 vs 多集群部署

**单集群方法：**

- 更简单的操作和统一管理
- 共享资源池，利用率更高
- 适合 500-1000 节点以下的组织
- 运营开销较低

**多集群方法：**

- 环境隔离（开发/预发/生产）
- 地理分布以降低延迟
- 监管合规（数据主权）
- 爆炸半径限制
- 适合大型组织（1000+ 节点）

### Kubernetes AI 核心组件

**资源管理：**

- **Device Plugins**: GPU/NPU 设备发现和分配
- **DRA（动态资源分配）**: 下一代灵活资源管理
- **Node Feature Discovery**: 硬件能力检测
- **Topology Manager**: NUMA 和设备拓扑感知

**调度：**

- **默认调度器**与评分插件
- **Kueue**: 多租户任务排队
- **Volcano**: 带 Gang 调度的批处理调度
- **Karpenter**: 即时节点配置

**工作负载管理：**

- **Job/CronJob**: 单次训练运行
- **JobSet**: 多任务协调（Alpha）
- **Kubeflow Training Operators**: 框架特定（PyTorch、TensorFlow）
- **LeaderWorkerSet**: 分布式推理编排

**存储：**

- **Persistent Volumes**: 模型存储和检查点
- **CSI Drivers**: 云原生存储集成
- **共享文件系统**: 训练数据分发（NFS、Lustre）
- **对象存储**: 模型工件和数据集（S3、GCS）

## Kubernetes 上 AI 的最佳实践

### 1. 合理配置基础设施规模

**不要为超大规模过度设计：**

- 从适当的集群规模开始（100-500 节点）
- 根据需要使用多个集群进行水平扩展
- 关注 GPU 利用率而不是原始节点数
- 在扩展前先监控和优化

### 2. 实施 GPU 共享

**最大化 GPU 投资回报：**

- 在 A100/H100 上使用 MIG 进行多租户隔离
- 为开发工作负载实施时间切片
- 考虑推理的分数 GPU 分配
- 监控每个 Pod 的 GPU 指标

### 3. 成本优化

**训练成本策略：**

- 使用 Spot/可抢占实例（节省 60-90%）
- 实施自动检查点
- 启用优先级调度的任务抢占
- 合理配置实例类型（不要总是使用最大的 GPU）

**推理成本策略：**

- 为适当的模型使用较小的 GPU 类型（T4、L4）
- 实施动态批处理
- 根据请求速率启用自动扩展
- 考虑小模型的 CPU 推理

### 4. 监控和可观测性

**要跟踪的关键指标：**

- 每个 Pod 的 GPU 利用率和内存使用
- 任务队列深度和调度延迟
- Pod 启动时间和镜像拉取持续时间
- 训练吞吐量（样本/秒）
- 推理延迟（p50、p95、p99）
- 每次训练运行和每次推理查询的成本

**工具：**

- **DCGM Exporter**: NVIDIA GPU 指标
- **Prometheus + Grafana**: 指标收集和可视化
- **Kubeflow Pipelines**: ML 工作流跟踪
- **MLflow/Weights & Biases**: 实验跟踪

### 5. 网络优化

**训练：**

- 为分布式训练启用 RDMA
- 使用拓扑感知调度（GPU + NIC 亲和性）
- 实施网络策略以确保安全
- 监控跨节点带宽

**推理：**

- 使用服务网格进行流量管理（Istio、Linkerd）
- 实施连接池
- 启用 HTTP/2 和 gRPC
- 考虑模型服务的边缘缓存

## 要避免的常见陷阱

1. **过度配置 GPU**: 在只需要一部分时请求完整 GPU
2. **忽略 Pod 启动时间**: 不优化镜像大小和拉取策略
3. **手动任务管理**: 不使用任务队列系统（Kueue、Volcano）
4. **单点故障**: 不为关键服务实施高可用性
5. **忽视成本监控**: 24/7 运行昂贵的 GPU 节点而不使用
6. **资源限制设置不当**: 不设置请求/限制导致节点不稳定
7. **检查点不足**: 在 Spot 中断时损失数小时的训练
8. **单体集群**: 混合所有工作负载类型而不隔离

## 展望：未来趋势

### 新兴技术

- **DRA GA**: 动态资源分配升级到稳定版
- **JobSet**: 复杂训练的多任务编排
- **原地 Pod 调整大小**: 无需重启调整资源
- **拓扑感知调度**: 更好的 GPU/NIC 放置
- **KEP-4817 Job 成功/完成策略**: 更好的任务生命周期管理

### 行业方向

- 推理专用集群的采用增加
- 多集群管理工具的增长
- 更好的 GPU 共享和虚拟化
- Kubernetes 上简化的 ML 平台抽象
- 与 Serverless 集成实现混合工作负载

## 结论

在 Kubernetes 上运行 AI 基础设施正在快速成熟，但调研数据显示，理想化的超大规模讨论与
实际部署之间存在差距。关键要点：

1. **大多数组织的运营规模远低于 1 万节点** - 针对实际规模进行优化
2. **GPU 利用率是 #1 挑战** - 实施共享和监控
3. **分离训练和推理** - 不同的需求需要不同的策略
4. **成本优化至关重要** - 使用 Spot 实例、合理配置和监控
5. **从简单开始，有意识地扩展** - 不要从第一天就过度设计

Kubernetes 生态系统继续发展，具有更好的 AI 原生特性。专注于为您的特定规模和需求
驱动 GPU 利用率和成本效率的实际改进。

## 多集群使用场景

不同的组织出于各种原因采用多集群策略。理解这些使用场景有助于制定架构决策：

<img src="../../../diagrams/multicluster-use-cases.png" alt="多集群使用场景" width="100%">

### 基于环境的分离（最常见）

**开发 → 预发 → 生产**

组织为每个环境部署单独的集群以：

- 安全测试部署而不破坏生产系统
- 保持生产稳定性和正常运行时间
- 在开发/预发环境中进行实验
- 为每个环境实施不同的安全策略

### 基于区域的部署

**每个区域一个集群以获得高性能**

地理分布提供：

- 为最终用户降低延迟
- 灾难恢复的地理冗余
- 跨区域更好的服务可用性
- 符合数据驻留要求

### 业务单元隔离

**每个业务单元一个集群以实现团队所有权**

单独的集群使能：

- 独立扩展和更新
- 团队自主性和所有权
- 简化合规性和审计
- 按团队分配预算

### 安全和管理的隔离

**工作负载隔离**

不同的隔离维度：

- 改善安全态势
- 简化管理和访问控制
- 更好的爆炸半径限制
- 更容易的故障排除和调试

### 基础设施提供商灵活性

**多云或混合部署**

跨提供商运行集群：

- 单云，多区域
- 多云以实现供应商独立性
- 本地和边缘位置
- 灵活部署避免锁定

这些模式通常结合使用 - 组织可能同时使用多种策略（例如，在多个区域部署，每个区域
都包含开发/预发/生产环境）。

## 参考资料

- 原文链接: <https://kube.today/ai-infrastructure-2025>
- [Kubernetes 大规模集群](../../kubernetes/large-scale-clusters.md)
- [GPU 调度和管理](../../kubernetes/scheduling-optimization.md)
- [Kubernetes 上的训练](../../training/README.md)
- [推理优化](../../inference/README.md)

---

*本文综合了行业调研、KubeCon 演讲和 Kubernetes 上真实 AI 基础设施部署的洞察。*
